name: Deploy boltfoundry-com

on:
  push:
    branches: [main]
    paths:
      - "apps/boltfoundry-com/**"
      - "infra/terraform/hetzner/**"
  workflow_dispatch:

# Prevent concurrent Terraform operations
concurrency:
  group: terraform-state
  cancel-in-progress: false

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write
    steps:
      - uses: actions/checkout@v4

      - uses: DeterminateSystems/nix-installer-action@main
      # - uses: DeterminateSystems/flakehub-cache-action@main  # Disabled FlakeHub caching

      - name: Build binary and sync production env
        run: |
          nix develop .#production --accept-flake-config --command bash -c "
            # Sync environment variables from vault accessible by service account
            # The OP_SERVICE_ACCOUNT_TOKEN determines which vault is used
            if [ -n \"${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}\" ]; then
              bft sitevar sync --force
            fi
            # Build the binary
            bft compile boltfoundry-com
          "
        env:
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}

      - name: Login to GitHub Container Registry
        run: echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Build and push Docker image
        run: |
          # Build with git hash and latest tags
          docker build \
            -t ghcr.io/${{ github.repository_owner }}/boltfoundry-com:${{ github.sha }} \
            -f infra/Dockerfile.deploy \
            --build-arg BINARY_PATH=build/boltfoundry-com \
            --build-arg BINARY_NAME=boltfoundry-com \
            .
          docker tag ghcr.io/${{ github.repository_owner }}/boltfoundry-com:${{ github.sha }} ghcr.io/${{ github.repository_owner }}/boltfoundry-com:latest
          docker push ghcr.io/${{ github.repository_owner }}/boltfoundry-com:${{ github.sha }}
          docker push ghcr.io/${{ github.repository_owner }}/boltfoundry-com:latest

      - name: Setup SSH
        run: |
          # First sync secrets from 1Password to get SSH_PRIVATE_KEY
          nix develop .#production --accept-flake-config --command bash -euc "
            export OP_SERVICE_ACCOUNT_TOKEN='${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}'
            # Sync all secrets from 1Password
            # Note: We only sync secrets here for SSH setup
            # Full sync happens in the deployment step
            bft sitevar sync --force --secret-only
          "

          # Now source the secrets file and setup SSH outside of Nix shell
          # This ensures the SSH key persists for subsequent steps
          source .env.secrets

          # Start SSH agent and add the key
          eval "$(ssh-agent -s)"

          # Setup SSH directory and key
          mkdir -p ~/.ssh

          # Write the SSH key to file
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa

          # Try to validate the key format
          echo "Validating SSH key format..."
          ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null 2>&1 && echo "Key format is valid" || echo "Key format is INVALID"

          # Verify the key is valid before proceeding
          if ! ssh-keygen -y -f ~/.ssh/id_rsa > /dev/null 2>&1; then
            echo "ERROR: SSH key format is invalid"
            echo "Key content first 50 chars: $(head -c 50 ~/.ssh/id_rsa)"
            echo "Key content last 50 chars: $(tail -c 50 ~/.ssh/id_rsa)"
            # Don't exit yet - let's try to continue without SSH agent
            echo "Will attempt to continue without SSH agent"
          else
            # Add the key to the SSH agent only if it's valid
            ssh-add ~/.ssh/id_rsa || echo "Failed to add key to SSH agent, continuing anyway"
          fi

          # Export SSH_AUTH_SOCK for subsequent steps
          echo "SSH_AUTH_SOCK=$SSH_AUTH_SOCK" >> $GITHUB_ENV

          # Note: ssh-keyscan will be done after we have the server IP
        env:
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Sync secrets and generate Kamal config from Terraform state
        id: terraform_output
        run: |
          # First sync secrets from 1Password
          nix develop .#production --accept-flake-config --command bash -euc "
            export OP_SERVICE_ACCOUNT_TOKEN='${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}'

            # Sync all secrets to .env.secrets
            bft sitevar sync --force

            # Source the secrets
            source .env.secrets

            # Export variables for Terraform
            export TF_VAR_hcloud_token=\$HETZNER_API_TOKEN
            export TF_VAR_cloudflare_api_token=\$CLOUDFLARE_API_TOKEN
            export TF_VAR_cloudflare_zone_id=\$CLOUDFLARE_ZONE_ID
            export TF_VAR_ssh_public_key=\$SSH_PUBLIC_KEY
            export TF_VAR_hyperdx_api_key=\$HYPERDX_API_KEY
            export TF_VAR_s3_access_key=\$AWS_ACCESS_KEY_ID
            export TF_VAR_s3_secret_key=\$AWS_SECRET_ACCESS_KEY
            export TF_VAR_github_token=\$GITHUB_PERSONAL_ACCESS_TOKEN
            export TF_VAR_hetzner_project_id=\$HETZNER_PROJECT_ID
            export TF_VAR_s3_endpoint=\$S3_ENDPOINT

            # Use CI credentials for Terraform backend
            export AWS_ACCESS_KEY_ID=\$TERRAFORM_BACKEND_ACCESS_KEY_ID
            export AWS_SECRET_ACCESS_KEY=\$TERRAFORM_BACKEND_SECRET_ACCESS_KEY

            # Get server IP from Terraform state (read-only operation)
            cd infra/terraform/hetzner
            # Map backend endpoint to S3 endpoint for consistency
            export TERRAFORM_S3_ENDPOINT=\$TERRAFORM_BACKEND_ENDPOINT
            terraform init -backend-config=\"endpoint=\$TERRAFORM_S3_ENDPOINT\"

            # Just read the floating IP from state, don't modify anything
            FLOATING_IP=\$(terraform output -raw server_ip 2>/dev/null || echo '')

            if [ -z \"\$FLOATING_IP\" ]; then
              echo \"âŒ Could not get server IP from Terraform state\"
              echo \"\"
              echo \"This usually means the infrastructure hasn't been deployed yet.\"
              echo \"Please ensure the infrastructure workflow has run successfully first:\"
              echo \"  1. Go to Actions â†’ Deploy Infrastructure (Hetzner)\"
              echo \"  2. Click 'Run workflow' and deploy the infrastructure\"
              echo \"  3. Once successful, re-run this deployment workflow\"
              echo \"\"
              echo \"Alternatively, if infrastructure was recently destroyed, you'll need to redeploy it first.\"
              exit 1
            fi

            # Store the IP for the deployment step
            echo \"BOLTFOUNDRY_COM_SERVER_IP=\$FLOATING_IP\" >> .env.secrets

            # Output the IP for GitHub Actions to use in next steps
            echo \"server_ip=\$FLOATING_IP\" >> \$GITHUB_OUTPUT
          "

          # Add server to known_hosts outside of Nix shell to ensure it persists
          # Use the server IP from the output
          ssh-keyscan -H ${{ steps.terraform_output.outputs.server_ip }} >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Install Kamal
        run: sudo gem install kamal --version 2.7.0 --no-document

      - name: Check for stale deploy locks
        id: check-lock
        run: |
          echo "Checking for existing deploy locks..."
          SERVER_IP="${{ steps.terraform_output.outputs.server_ip }}"

          # Setup SSH for this check
          nix develop .#production --accept-flake-config --command bash -c "
            export OP_SERVICE_ACCOUNT_TOKEN='${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}'
            bft sitevar sync --force
            source .env.secrets

            mkdir -p ~/.ssh
            echo \"\$SSH_PRIVATE_KEY\" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa

            echo \"Host $SERVER_IP\" >> ~/.ssh/config
            echo \"  StrictHostKeyChecking no\" >> ~/.ssh/config
            echo \"  UserKnownHostsFile /dev/null\" >> ~/.ssh/config
            chmod 600 ~/.ssh/config
          "

          # Check if lock exists and get its age
          if ssh root@$SERVER_IP "test -d .kamal/lock-boltfoundry-com"; then
            echo "Deploy lock found. Checking age..."

            # Get lock creation time
            LOCK_TIME=$(ssh root@$SERVER_IP "stat -c %Y .kamal/lock-boltfoundry-com" || echo "0")
            CURRENT_TIME=$(date +%s)
            LOCK_AGE=$((CURRENT_TIME - LOCK_TIME))

            # If lock is older than 30 minutes (1800 seconds), consider it stale
            if [ $LOCK_AGE -gt 1800 ]; then
              echo "Lock is older than 30 minutes ($LOCK_AGE seconds). Removing stale lock..."
              ssh root@$SERVER_IP "rm -rf .kamal/lock-boltfoundry-com"
              echo "âœ… Stale lock removed"
            else
              echo "Lock is recent ($LOCK_AGE seconds old). Checking if deployment is active..."

              # Check if any kamal process is running
              if ! ssh root@$SERVER_IP "pgrep -f kamal > /dev/null 2>&1"; then
                echo "No active Kamal process found. Removing orphaned lock..."
                ssh root@$SERVER_IP "rm -rf .kamal/lock-boltfoundry-com"
                echo "âœ… Orphaned lock removed"
              else
                echo "âŒ Active deployment in progress. Cannot proceed."
                echo "To force unlock, run the 'Unlock Kamal Deploy' workflow manually."
                exit 1
              fi
            fi
          else
            echo "No existing deploy lock found âœ…"
          fi

      - name: Deploy with Kamal
        run: |
          # Sync production config and secrets from 1Password and generate Kamal config
          nix develop .#production --accept-flake-config --command bash -c "
            export OP_SERVICE_ACCOUNT_TOKEN='${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}'

            # Sync ALL variables (both config and secrets) from 1Password
            bft sitevar sync --force

            # Restore the server IP that was retrieved from Terraform
            echo \"BOLTFOUNDRY_COM_SERVER_IP=${{ steps.terraform_output.outputs.server_ip }}\" >> .env.secrets

            # Generate Kamal config dynamically based on available secrets
            bft generate-kamal-config infra/terraform/hetzner/deploy.yml.tpl config/deploy.yml production

            # Create .env file for Kamal with config values only
            # Secrets are handled separately via .kamal/secrets for security
            cp .env.config .env
          "

          # Create .kamal directory and secrets file for Kamal 2.x
          mkdir -p .kamal

          # Start with the GitHub token for registry authentication
          echo "GITHUB_TOKEN=${KAMAL_REGISTRY_PASSWORD}" > .kamal/secrets

          # Copy all runtime secrets from .env.secrets to .kamal/secrets
          # This ensures Kamal has access to all secrets referenced in the config
          # Note: Config values from .env.config are passed via .env file
          if [ -f .env.secrets ]; then
            # Source the secrets and add them to .kamal/secrets
            # Skip infrastructure-only secrets that aren't needed at runtime
            source .env.secrets
            for var in $(grep "^[A-Z_]*=" .env.secrets | cut -d= -f1); do
              # Skip infrastructure and deployment secrets
              case "$var" in
                SSH_PRIVATE_KEY|SSH_PUBLIC_KEY|HETZNER_API_TOKEN|CLOUDFLARE_*|TERRAFORM_*|AWS_ACCESS_KEY_ID|AWS_SECRET_ACCESS_KEY|GITHUB_PERSONAL_ACCESS_TOKEN|HETZNER_PROJECT_ID|S3_ENDPOINT|*_SERVER_IP)
                  # Skip these - not needed at runtime
                  ;;
                *)
                  # Add runtime secret to .kamal/secrets
                  eval "value=\$$var"
                  if [ ! -z "$value" ]; then
                    echo "$var=$value" >> .kamal/secrets
                  fi
                  ;;
              esac
            done
          fi

          chmod 600 .kamal/secrets

          # Set up SSH for this step since GitHub Actions doesn't persist SSH agent between steps
          # We need to ensure the SSH key is available for Kamal
          if [ -f ~/.ssh/id_rsa ]; then
            echo "SSH key already exists at ~/.ssh/id_rsa"
            echo "Key fingerprint: $(ssh-keygen -lf ~/.ssh/id_rsa)"
          else
            echo "Setting up SSH key for Kamal deployment"
            source .env.secrets
            mkdir -p ~/.ssh
            echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa
            echo "Created SSH key with fingerprint: $(ssh-keygen -lf ~/.ssh/id_rsa)"
          fi

          # Test SSH connection directly
          echo "Testing SSH connection to ${{ steps.terraform_output.outputs.server_ip }}..."
          ssh -o BatchMode=yes -o ConnectTimeout=5 root@${{ steps.terraform_output.outputs.server_ip }} "echo 'SSH connection successful'" || echo "Direct SSH test failed"

          # Ensure StrictHostKeyChecking is disabled for the deployment
          echo "Host ${{ steps.terraform_output.outputs.server_ip }}" >> ~/.ssh/config
          echo "  StrictHostKeyChecking no" >> ~/.ssh/config
          echo "  UserKnownHostsFile /dev/null" >> ~/.ssh/config
          chmod 600 ~/.ssh/config

          # Deploy with Kamal 2.x (will use .env for secret environment variables)
          # Use verbose mode to debug SSH issues
          kamal deploy --verbose
        env:
          KAMAL_REGISTRY_PASSWORD: ${{ secrets.GITHUB_TOKEN }}
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.PRODUCTION_OP_SERVICE_ACCOUNT_TOKEN }}
          BOLTFOUNDRY_COM_SERVER_IP: ${{ steps.terraform_output.outputs.server_ip }}
        working-directory: ./

      - name: Validate deployment health
        run: |
          echo "ğŸ” Validating deployment health..."

          # Wait for deployment to stabilize
          echo "â³ Waiting for deployment to stabilize..."
          sleep 30

          # Health check with retries
          echo "ğŸ©º Testing health endpoint..."
          for i in {1..60}; do
            if curl -f -L -s https://boltfoundry.com/ > /dev/null 2>&1; then
              echo "âœ… Health endpoint responded successfully"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "âŒ Health endpoint failed after 60 attempts (10 minutes)"
              echo "ğŸ”„ Attempting rollback..."
              kamal rollback --version=latest || echo "âš ï¸ Rollback command failed"
              exit 1
            fi
            echo "Health check attempt $i/60 failed, retrying in 10s..."
            sleep 10
          done

          # Test main page endpoint
          echo "ğŸ  Testing main page endpoint..."
          for i in {1..10}; do
            response_code=$(curl -L -s -o /dev/null -w "%{http_code}" https://boltfoundry.com/)
            if [ "$response_code" = "200" ]; then
              echo "âœ… Main page responded with 200 OK"
              break
            fi
            if [ $i -eq 10 ]; then
              echo "âŒ Main page failed with response code: $response_code"
              echo "ğŸ”„ Attempting rollback..."
              kamal rollback --version=latest || echo "âš ï¸ Rollback command failed"
              exit 1
            fi
            echo "Main page attempt $i/10 failed (response: $response_code), retrying in 10s..."
            sleep 10
          done

          # Enhanced health check - get detailed health info
          echo "ğŸ”¬ Performing detailed health check..."
          health_response=$(curl -L -s https://boltfoundry.com/)
          if echo "$health_response" | grep -q 'html\|HTML'; then
            echo "âœ… Detailed health check passed"
            echo "Health response contains HTML content"
          else
            echo "âŒ Detailed health check failed - invalid response format"
            echo "Response: $health_response"
            echo "ğŸ”„ Attempting rollback..."
            kamal rollback || echo "âš ï¸ Rollback command failed"
            exit 1
          fi

          echo "ğŸ‰ Deployment validation successful!"

      - name: Monitor post-deployment stability
        run: |
          echo "ğŸ‘€ Monitoring deployment stability for 2 minutes..."

          # Monitor for 2 minutes to catch early failures
          for i in {1..12}; do
            response_code=$(curl -L -s -o /dev/null -w "%{http_code}" https://boltfoundry.com/)
            if [ "$response_code" != "200" ]; then
              echo "âŒ Stability check failed with response code: $response_code at check $i/12"
              echo "ğŸ”„ Attempting rollback..."
              kamal rollback --version=latest || echo "âš ï¸ Rollback command failed"
              exit 1
            fi
            echo "âœ… Stability check $i/12 passed (response: $response_code)"
            sleep 10
          done

          echo "ğŸ¯ Deployment is stable and healthy!"

      - name: Deployment success notification
        if: success()
        run: |
          echo "ğŸš€ Deployment completed successfully!"
          echo "âœ… Service URL: https://boltfoundry.com"
          echo "âœ… Health Check: https://boltfoundry.com/"
          echo "âœ… Deployment validated and stable"

      - name: Deployment failure notification
        if: failure()
        run: |
          echo "ğŸ’¥ Deployment failed!"
          echo "âŒ Service may be unresponsive at: https://boltfoundry.com"
          echo "ğŸ” Check logs above for specific failure reason"
          echo "âš ï¸ Manual intervention may be required"
